## 引言

在之前的学习中，我们通常将整个通信系统视为一个黑盒，重点关注如何将信号（`signalling`）通过信道（`channel`）可靠地传输，并在接收端进行检测（`detection`）。



本章，我们将聚焦于通信系统的**起点**——**信源 (Source)**。我们的核心任务是研究如何对信源产生的信息进行高效、紧凑的表示，这个过程统称为**信源编码**。

信源主要分为两类，本章也将围绕这两类展开：

1.  **数字信源** 💻：其输出是离散的符号序列。我们的目标是**压缩 (Compression)** 它，去除冗余，而压缩的理论极限由**熵 (Entropy)** 决定。
2.  **模拟信源** 🎤：其输出是连续的时间波形。我们需要先将其**数字化 (Digitization)**，这个过程包括**采样、量化、编码**。



> ✨ **本章重点**：
>
> *   ① **熵 (Entropy)**：信息的不确定性的度量，数据压缩的理论基石。
> *   ② **量化 (Quantization)**：将模拟信号数字化的核心步骤之一。

---

## 1. 数字信源与信息度量

### 1.1 数字信源模型 🌱

**信源**可以被看作产生信息的源头，可以分为两大类：

*   **模拟信源 (Analog Source)**：输出是**时间连续、取值连续**的波形，例如麦克风拾取的声音信号。
*   **数字信源 (Digital Source)**：也叫离散信源，输出是**时间离散、取值离散**的符号序列，例如计算机中的文本文件。

本节我们首先关注**数字信源**。一个数字信源的输出是一个符号串，其中每个符号都取自一个有限的**字符集 (Alphabet)** $\Omega$。

*   **例子**：
    *   一个二进制信源的字符集是 $\Omega = \{0, 1\}$。
    *   英文文章的字符集是 $\{a, b, ..., z, A, B, ..., Z, \text{标点符号}\}$。
    *   中文文章的字符集是常用汉字和标点符号的集合。

一个数字信源输出的符号序列可以表示为 $X_0, X_1, X_2, \dots$，其中每个符号 $X_i \in \Omega$。

为简化分析，我们通常做如下**默认假设**：

> **序列是平稳的 (Stationary)**：序列中每个符号的统计特性（即概率分布）都是相同的。

因此，我们可以用一个代表任意位置的随机变量 $X$ 及其概率分布来描述整个信源的统计特性。若字符集 $\Omega = \{x_1, x_2, \dots, x_n\}$，其概率分布可以表示为：

$$
\left( \begin{array}{c} X \\ P(X=x_i) \end{array} \right) = \left( \begin{array}{cccc} x_1 & x_2 & \dots & x_n \\ P(x_1) & P(x_2) & \dots & P(x_n) \end{array} \right)
$$

*   **示例**：
    *   **二进制等概信源**：$X \in \{0, 1\}$，其概率分布为 $\left( \begin{array}{cc} 0 & 1 \\ 1/2 & 1/2 \end{array} \right)$。
    *   **均匀骰子**：$X \in \{1, 2, \dots, 6\}$，其概率分布为 $\left( \begin{array}{cccccc} 1 & 2 & 3 & 4 & 5 & 6 \\ 1/6 & 1/6 & 1/6 & 1/6 & 1/6 & 1/6 \end{array} \right)$。

我们可以将**一串符号**看作一个**更大的复合符号**。例如，独立掷3次非均匀硬币（正面概率为 $p$），得到的序列 $Z = (X_1, X_2, X_3)$ 可以看作一个8进制的复合符号，其字符集为 $\Omega_Z = \{000, 001, \dots, 111\}$。其中一个特定序列的概率为：

$$
P(Z=001) = P(X_1=0, X_2=0, X_3=1) = p \cdot p \cdot (1-p) = p^2(1-p)
$$

### 1.2 信息的度量与熵 🧐

#### 1.2.1 信息的直观理解与自信息

信息究竟是什么？如何度量信息量的大小？

*   **核心思想**：信息是**不确定性的消除**。一件事情在发生前越不确定（概率越小），当它发生时，带来的信息量就越大。
    *   “太阳从东边升起” -> 几乎没有信息量（概率~1）。
    *   “彩票中了头奖” -> 信息量巨大（概率极小）。

*   **性质**：
    1.  信息量 $I$ 是概率 $P(A)$ 的函数， $P(A) \in [0, 1] \to I_A \in [0, \infty)$。概率越小，信息量越大。
    2.  两个**独立**事件同时发生，其总信息量应该是两者之和。$P(A, B) = P(A)P(B) \to I_{AB} = I_A + I_B$。

唯一满足以上性质的函数形式是**对数函数**。我们定义事件A的**自信息 (Self-Information)** 为：

$$
\boxed{I(A) = -\log P(A)}
$$

*   **对数的底 (Base)**：
    *   底为2：单位是**比特 (bit)** 或 **香农 (Shannon)**。
    *   底为e：单位是**奈特 (nat)**。
    *   底为10：单位是**哈特 (hart)** 或 **迪特 (det)**。
*   在通信和计算机科学中，**默认底为2**。

> **一个Yes/no问题代表1比特信息**。信息量的比特数，可以理解为为了确定最终结果，最少需要问多少个“对半开”的是非问题。

#### 1.2.2 熵 (Entropy) 的定义

一个信源会输出各种可能的符号，每个符号都带有自己的自信息。那么如何衡量整个信源的**平均信息量**呢？答案就是**熵 (Entropy)**。

熵是信源所有可能输出符号的**自信息的统计平均值 (数学期望)**。

对于一个离散信源 $X$，其字符集为 $\{x_1, \dots, x_n\}$，对应概率为 $\{p_1, \dots, p_n\}$，其熵 $H(X)$ 定义为：

$$
H(X) = E[I(X)] = E[-\log p(X)]
$$

$$
\boxed{H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i \quad (\text{bit/symbol})}
$$

> **注意**：对于概率为零的项，我们定义 $0 \log 0 = 0$。

*   **示例1：某班有30人，20人姓杨，10人姓牛。随机抽取一人。**
    *   抽到“杨”的概率是 $2/3$，自信息是 $-\log_2(2/3)$。
    *   抽到“牛”的概率是 $1/3$，自信息是 $-\log_2(1/3)$。
    *   该事件的熵为：
        $$
        H(X) = -\left(\frac{2}{3}\log_2\frac{2}{3} + \frac{1}{3}\log_2\frac{1}{3}\right) = \log_2 3 - \frac{2}{3} \approx 0.918 \text{ bit}
        $$

#### 1.2.3 联合熵与条件熵

*   **联合熵 (Joint Entropy)**：衡量一对随机变量 $(X, Y)$ 的平均信息量。
    $$
    H(X, Y) = -\sum_{i} \sum_{j} P(x_i, y_j) \log_2 P(x_i, y_j)
    $$

*   **条件熵 (Conditional Entropy)**：在已知 $Y$ 的条件下，$X$ 的剩余不确定性（信息量）的平均值。
    $$
    \begin{aligned}
    H(X|Y) &= \sum_{j} P(y_j) H(X|Y=y_j) \\
    &= \sum_{j} P(y_j) \left[ -\sum_{i} P(x_i|y_j) \log_2 P(x_i|y_j) \right] \\
    &= -\sum_{i} \sum_{j} P(x_i, y_j) \log_2 P(x_i|y_j)
    \end{aligned}
    $$

它们之间存在重要的**链式法则 (Chain Rule)**：

$$
\boxed{H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)}
$$

这个公式的直观理解是：$(X,Y)$ 的总不确定性 = $X$ 的不确定性 + 已知 $X$ 后 $Y$ 的剩余不确定性。

#### 1.2.4 熵的性质 ✨

1.  **非负性**：$H(X) \ge 0$。
2.  **确定性**：如果一个事件是确定的（概率为1），则其熵为0。
3.  **最大熵定理**：对于一个有 $M$ 个可能取值的信源，其熵的最大值为 $H(X) \le \log_2 M$。当且仅当所有取值**等可能**时，即 $p_i = 1/M$ 时，等号成立。
    *   *这个定理是数据压缩的理论基础之一。它告诉我们，一个信源最“混乱”、最“不可预测”的状态是均匀分布。*
4.  **条件熵不增**：$H(X|Y) \le H(X)$。
    *   *直观理解：获得额外的信息（知道 $Y$），不会增加原事件（$X$）的不确定性。通常会减少或保持不变。*
    *   当且仅当 $X$ 和 $Y$ **相互独立**时，等号成立，即 $H(X|Y) = H(X)$。

#### 1.2.5 二元熵函数

对于一个只有两个输出 $\{0, 1\}$，概率分别为 $\{p, 1-p\}$ 的二进制信源，其熵为：

$$
H(X) = -p \log_2 p - (1-p) \log_2 (1-p) \triangleq h_2(p)
$$

这个函数被称为**二元熵函数**，它的图像是一个向上凸起的曲线，在 $p=0.5$ 时达到最大值1。



### 1.3 互信息 (Mutual Information)

**互信息** $I(X;Y)$ 衡量的是一个随机变量 $Y$ 中包含的关于另一个随机变量 $X$ 的信息量。或者说，在得知 $Y$ 后，关于 $X$ 的不确定性的**减少量**。

*   **定义**：
    $$
    \boxed{
    \begin{aligned}
    I(X;Y) &= H(X) - H(X|Y) \\
    &= H(Y) - H(Y|X) \\
    &= H(X) + H(Y) - H(X,Y)
    \end{aligned}
    }
    $$

*   **意义**：
    *   $H(X)$：在观察 $Y$ **之前**，我对 $X$ 的无知程度。
    *   $H(X|Y)$：在观察 $Y$ **之后**，我对 $X$ 的无知程度。
    *   $I(X;Y)$：观察 $Y$ 这件事，**消除**了多少我对 $X$ 的无知。

*   **性质**：
    1.  **对称性**：$I(X;Y) = I(Y;X)$。
    2.  **非负性**：$I(X;Y) \ge 0$ (因为 $H(X) \ge H(X|Y)$)。
    3.  **独立性**：当且仅当 $X, Y$ 相互独立时，$I(X;Y) = 0$。

互信息是通信理论中一个极其重要的概念，它直接关联到**信道容量**——信道能够可靠传输的最大信息速率。

### 1.4 熵与互信息的关系 (文氏图) 👍

我们可以用文氏图来非常直观地理解熵、联合熵、条件熵和互信息之间的关系。



*   **$H(X)$ 和 $H(Y)$**：分别代表两个圆的面积（各自的不确定性）。
*   **$H(X,Y)$**：两个圆的**并集**面积（总不确定性）。
*   **$I(X;Y)$**：两个圆的**交集**面积（共同的信息，不确定性的减少量）。
*   **$H(X|Y)$**：$H(X)$ 圆中**除去**交集的部分（已知Y后，X的剩余不确定性）。
*   **$H(Y|X)$**：$H(Y)$ 圆中**除去**交集的部分（已知X后，Y的剩余不确定性）。

从图中可以清晰地看出：$H(X,Y) = H(X|Y) + H(Y|X) + I(X;Y)$。

---

## 2. 模拟信源的数字化

现实世界中大量的信源是模拟的，如语音、图像。为了利用数字通信系统的强大优势，我们必须首先将模拟信号转换为数字信号。这个过程包括三个关键步骤：



1.  **采样 (Sampling)**：在时间上将连续信号离散化。
2.  **量化 (Quantization)**：在幅度上将连续取值映射为离散电平。
3.  **编码 (Coding)**：将离散的量化电平表示为二进制码字。

> 🧠 **知识回顾**：根据**奈奎斯特采样定理**，为了无失真地从采样值中恢复原信号，采样频率 $f_s$ 必须大于或等于信号最高频率 $f_H$ 的两倍，即 $f_s \ge 2f_H$。

本节重点讨论**量化**。

### 2.1 量化 (Quantization)

量化是一个将连续的采样值 $x$ 映射到一个有限集合中的离散值 $\hat{x}$ 的过程。这个过程可以用一个函数 $\hat{x} = Q(x)$ 来表示，其图像是一个阶梯状函数。



*   **关键参数**：
    *   **动态范围 $[-A, +A]$**：量化器能处理的输入信号的最大幅度范围。
    *   **量化级数 $M$**：离散输出电平的总个数。
    *   **量化比特数 $b$**：表示一个量化电平所需的二进制位数，$M=2^b$。
    *   **量化电平 $y_k$**：代表一个量化区间的输出值。
    *   **量化边界 $x_k$**：划分不同量化区间的临界值。
    *   **量化区间 $(x_{k-1}, x_k)$**：落入此区间的输入都会被映射到同一个量化电平 $y_k$。

#### 均匀量化器

最简单的量化器是**均匀量化器**，其特点是：
*   所有量化**间隔**都相等：$\Delta = x_k - x_{k-1} = \frac{2A}{M}$。
*   量化**电平**是区间的**中点**：$y_k = \frac{x_k + x_{k-1}}{2}$。



### 2.2 量化噪声与信噪比 (SQNR)

量化是**有损**的过程，它引入了误差。**量化误差**定义为输出与输入之差：

$$
e_q = \hat{x} - x = y - x
$$

这个误差可以看作是叠加在原始信号上的**量化噪声**。我们用**量化信噪比 (Signal-to-Quantization-Noise Ratio, SQNR)** 来衡量量化性能的好坏。

$$
SQNR = \frac{S}{N_q} = \frac{E[x^2]}{E[e_q^2]}
$$

其中 $S$ 是信号功率，$N_q$ 是量化噪声功率。

对于**均匀量化器**，如果输入信号在 $[-A, +A]$ 内**均匀分布**，我们可以推导出一个非常重要的工程近似公式：

*   信号功率 $S = \frac{A^2}{3}$。
*   量化噪声功率 $N_q = \frac{\Delta^2}{12} = \frac{(2A/M)^2}{12} = \frac{A^2}{3M^2}$。
*   因此，SQNR为：
    $$
    SQNR = \frac{A^2/3}{A^2/(3M^2)} = M^2 = (2^b)^2 = 2^{2b}
    $$

将其转换为分贝 (dB) 表示：

$$
\boxed{SQNR_{dB} = 10 \log_{10}(M^2) = 20 \log_{10}(2^b) = 20b \log_{10}2 \approx 6.02b \text{ dB}}
$$

> ✨ **重要结论**：对于均匀量化，**量化比特数每增加1 bit，量化信噪比提高约6 dB**。这是数字音频和数字通信中的一个黄金法则。

*   **示例**：标准CD音质是16 bit量化，其理论SQNR约为 $16 \times 6 = 96$ dB。电话语音采用8 bit量化，其SQNR约为 $8 \times 6 = 48$ dB。

### 2.3 最佳量化器 (Lloyd-Max Quantizer)

对于非均匀分布的信号（如语音信号，小幅度出现的概率远大于大幅度），均匀量化器并非最佳选择。**最佳量化器**的目标是在给定量化级数 $M$ 的条件下，最小化量化噪声功率 $N_q$。

通过对 $N_q$ 求导并使其为零，可以得到最佳量化器必须满足的两个条件（**Lloyd-Max条件**）：

1.  **量化边界** $x_k$ 必须是相邻两个**量化电平** $y_k$ 和 $y_{k+1}$ 的**中点**。
    $$
    x_k = \frac{y_k + y_{k+1}}{2}
    $$
2.  **量化电平** $y_k$ 必须是其对应**量化区间** $(x_{k-1}, x_k)$ 内信号的**概率质心（条件期望）**。
    $$
    y_k = E[x | x \in (x_{k-1}, x_k)] = \frac{\int_{x_{k-1}}^{x_k} x \cdot p(x) dx}{\int_{x_{k-1}}^{x_k} p(x) dx}
    $$

> **结论**：
> *   对于**均匀分布**的输入信号，**均匀量化器**就是最佳量化器。
> *   对于**非均匀分布**的信号，最佳量化器是一个**非均匀量化器**，其量化间隔在信号概率密度大的地方更密集，在概率密度小的地方更稀疏。语音编码中常用的A律/μ律对数压缩编码，就是一种对最佳非均匀量化的近似。

---

## 3. 复用技术

复用 (Multiplexing) 技术允许多路信号共享同一个物理信道，从而极大地提高信道利用率。

### 3.1 时分复用 (Time Division Multiplexing, TDM)

TDM 将时间轴划分为连续的**帧 (Frame)**，再将每一帧划分为若干个**时隙 (Time Slot)**。每个时隙分配给一个用户（信源）。



*   **原理**：各路信号在时间上**轮流**占用信道。
*   **速率**：理想情况下，TDM复用后的总输出速率等于所有输入支路速率之和。
    $$
    R_{total} = R_1 + R_2 + \dots + R_N
    $$
*   **应用**：数字电话系统（PCM）是TDM的经典应用。例如，欧洲的E1标准将32路64kbps的电话语音复用成一路2.048Mbps的数据流（其中30路用于话音，2路用于同步和信令）。

### 3.2 频分复用 (Frequency Division Multiplexing, FDM)

FDM 将可用的信道总带宽划分为若干个互不重叠的**子频带**，每个子频带分配给一个用户。



*   **原理**：各路信号在频率上**并行**占用不同的“车道”。
*   **应用**：广播电视、早期的移动通信（1G）等。