# **机器人主动感知：下一最佳视点规划**

## **第一部分：下一最佳视点规划的基础**

### **第1节 主动感知范式**

#### **1.1. 定义下一最佳视点（NBV）问题**

下一最佳视点（Next-Best-View, NBV）规划是主动视觉（Active Vision）领域的一个基本问题，其核心是根据至今已收集到的所有信息，迭代地确定传感器观察环境或物体的下一个最具信息量的位姿（即视点）1。该问题最早由Connolly提出3，其根本挑战在于选择能够为现有模型带来“最佳”改进的“下一个”视点5。与被动观察范式相比，NBV通过实现灵活和智能的信息采集，构成了主动感知能力的核心6。它使机器人能够自主探索和重建其环境，从而显著增强其自主性与效率1。

#### **1.2. 核心目标与权衡**

NBV规划的首要目标是在最小化操作成本的同时，最大化“信息增益”1。这是一个复杂的多目标优化问题，涉及关键的权衡。

* **信息增益（Information Gain）**：这是NBV规划的中心度量标准，通常量化为在新视点下收集到的新信息量或对环境不确定性的降低程度1。具体而言，信息增益可以表现为发现未知的表面8、降低概率地图（如占用栅格地图）的熵7，或是提升三维重建模型的质量与完整性9。  
* **成本最小化（Cost Minimization）**：这是一个多维度的约束条件，需要在信息增益与多种操作成本之间进行权衡1。关键成本因素包括：  
  * **运动成本**：机器人移动到下一个视点所需的时间、能量消耗或行进距离1。  
  * **时间与视点数量**：完成任务所需的总时间或总视点数量，这直接影响系统的整体效率11。  
  * **计算开销**：规划下一个最佳视点所需的计算资源与时间，这对于实时应用至关重要2。

一个最优的NBV策略必须在潜在的信息增益与这些成本之间找到平衡点1。这一过程从根本上体现了人工智能领域经典的“探索-利用”（Exploration-Exploitation）权衡，但在一个物理的、具身化的环境中展开。在此情境下，“探索”对应于发现全新的、未曾观测过的表面或空间，而“利用”则对应于优化已知区域的模型，例如增加已有表面的点云密度以提升细节5。成本函数（如运动和时间）则构成了对这一决策过程的预算约束。因此，选择下一个视点不仅仅是回答“哪里最未知？”，而是回答“在给定的成本预算下，我能获得的最有价值的信息是什么？”。这个视角的转变将NBV从一个纯粹的几何问题，重新定义为一个核心的AI决策挑战。

#### **1.3. 重要性与应用**

NBV规划对于提升机器人系统的自主性与效率至关重要1，其应用领域广泛且影响深远。

* **三维重建与地图构建**：这是NBV最经典和广泛的应用。通过策略性地选择视点，机器人能够高效地创建精确、完整的三维模型，这些模型对于导航、定位、场景理解和数字孪生等任务至关重要7。  
* **物体识别与场景理解**：NBV通过有目的地收集数据，帮助机器人更准确地识别物体，理解物体间的空间关系，并最终构建对场景的语义认知2。  
* **机器人操纵**：在杂乱无章的场景中，NBV能够为机器人操纵提供决策支持。通过主动观察，机器人可以获取物体精确的位姿、形状以及抓取点（affordance）信息，从而规划出更可靠的抓取策略1。  
* **巡检与监视**：NBV被用于巡检复杂的工业设施（如建筑物、桥梁、油气管道）或执行监视任务，在这些应用中，全面的覆盖率是成功的关键7。

#### **1.4. 影响因素**

NBV规划器的设计和性能受到多个环境与系统因素的深刻影响。

* **环境复杂度**：在杂乱、动态或大规模的环境中，存在大量的遮挡，这要求NBV算法具备更高的鲁棒性和更复杂的规划能力，以安全地导航并获取有效信息1。  
* **传感器能力**：传感器的类型（如RGB-D相机、激光雷达LiDAR）、质量（如分辨率、噪声水平）以及视场角（Field-of-View, FoV）直接决定了单次观测能获取的信息量和质量，从而影响规划策略1。  
* **机器人运动学**：机器人的移动能力（如地面车辆、无人机）和操纵能力（如机械臂）共同定义了其可达视点的范围，并直接影响运动成本的计算1。

随着应用场景和任务需求的多样化，“信息”本身的定义也在不断演进，这成为NBV方法论多样化的核心驱动力。最初，在以三维重建为主要目标的应用中，“信息”被严格地从几何角度定义，例如新观测到的未知空间体积或表面点数8。然而，当任务转变为机器人抓取时，“信息”的定义随之改变，转变为对一个可靠抓取姿态的置信度6。在这种情况下，一个能够揭示物体远端大量新点云的视点，如果对于确定近端抓取点毫无帮助，那么其信息价值就非常低。近年来，随着神经辐射场（NeRF）等新三维表示方法的兴起，“信息”的定义再次演变，开始包含光度学质量，即一个能够最大程度减少最终渲染图像伪影的视点被认为是信息量大的15。这意味着不存在一个普适的“最优”NBV算法，效用函数和信息定义必须与下游任务协同设计，这也预示了领域向任务特定NBV规划器发展的趋势。

### **第2节 经典方法：NBV的基础**

在深度学习浪潮之前，NBV规划主要由两大类经典方法主导：基于信息论的方法和基于几何驱动的方法。这些方法为后来的研究奠定了坚实的基础。

#### **2.1. 信息论范式**

该范式将NBV问题视为在一个概率性世界模型中减少不确定性的过程。

* **核心原理**：首先，建立一个环境的概率表示，通常是基于体素的占用栅格地图（Occupancy Grid Map）。然后，选择那个预期能够最大程度降低整个地图熵（即不确定性）的视点2。  
* **实现方式**：环境被离散化为一个个的体素（voxel），每个体素都有一个状态（如：被占用、空闲、未知）和相应的概率18。系统会生成一系列候选视点，并对每个候选视点使用一个效用函数来评估其预期的信息增益。例如，计算从该视点观测后，有多少“未知”体素能够被确认为“被占用”或“空闲”3。  
* **光线投射的瓶颈**：这种方法的一个关键且致命的环节是光线投射（ray-casting），它被用来模拟从候选视点发出的光线，以判断每个体素的可见性3。这个过程的计算量巨大，并且随着环境尺寸和地图分辨率的增加而急剧恶化，成为该范式的一个主要性能瓶颈3。为了缓解这一问题，后续研究探索了基于投影的近似计算方法3。

#### **2.2. 几何驱动范式**

与信息论方法不同，几何驱动范式直接在已观测到的表面几何属性上进行操作，通常不依赖显式的概率模型。

* **核心原理**：假设当前的部分观测是精确的，然后利用几何线索来引导机器人探索那些能够补全整个表面的区域14。  
* **主要方法**：  
  * **基于边界的探索（Frontier-Based Exploration）**：该方法首先识别出“边界”（frontiers），即已知自由空间与未知空间的交界处。NBV被选择为能够将传感器移动到这些边界区域的视点，从而扩展已知区域14。  
  * **表面边缘探索（Surface Edge Explorer, SEE）**：该方法根据局部点云的测量密度对点进行分类，然后选择能够专门观测那些采样不足的边缘或表面的视点，以提高覆盖均匀性5。SEE使用非结构化的点云表示，避免了体素化过程中可能导致的信息损失5。  
  * **基于遮挡的方法（Occlusion-Based Methods）**：这类方法显式地推理当前视点中的遮挡关系，并选择一个最有可能看到被遮挡物体或表面后方的视点5。

#### **2.3. 基于搜索与基于合成的方法**

经典方法在确定最终视点的方式上，可以分为两大类。

* **基于搜索（Search-Based）**：这是最常见的策略。系统首先生成一个离散的候选视点集合（例如，在物体周围的球面上均匀采样，或根据几何特征生成），然后使用效用函数对每个候选视点进行评估，最终选择得分最高的那个8。这种方法的主要挑战在于如何生成一个高质量的候选集，同时避免搜索空间过大而导致计算不可行3。  
* **基于合成（Synthesis-Based）**：这类方法试图直接计算或“合成”出最优的视点位姿，而无需进行显式的搜索。这通常需要借助优化算法来实现6。

信息论与几何驱动方法之间的分野，反映了NBV领域一个根本性的设计抉择：是建模**体素不确定性**还是**表面不确定性**。前者通过如占用栅格地图等体素化表示，旨在厘清每一寸**空间**的状态（是空是实？），这对于需要进行障碍物规避和导航的任务极为有利，但其代价是高昂的内存消耗和计算成本13。后者则常采用点云或网格等表面表示，其目标是补全物体的

**表面**，这种方式内存效率更高，且与重建质量直接相关，但它无法描述自由空间，因此在杂乱环境中的导航能力较弱5。体素化方法中光线投射的巨大计算瓶颈3，直接催生了更高效的几何方法5以及后来能够完全绕过显式模拟的学习方法的诞生。

此外，经典方法对人工设计的启发式规则和效用函数的高度依赖，导致了其固有的“脆弱性”。这些方法的性能对参数调整（如体素大小、候选视点采样策略）和特定环境的几何形状高度敏感5。例如，一个为巡检小型复杂物体而优化的系统，在测绘一个大型开放房间时可能表现不佳。这种缺乏适应性和需要为每个新领域进行专家调校的特性，极大地限制了它们的泛化能力，并直接激发了研究者们去探索更具适应性的、数据驱动的解决方案，从而引发了第二部分将要详述的范式转变。

## **第二部分：范式转变：基于学习的NBV**

随着深度学习的兴起，NBV规划领域经历了一场深刻的范式革命。研究人员开始利用数据驱动的方法来学习NBV策略，以克服经典方法在泛化性、计算效率和适应性方面的局限。下表对经典方法与新兴的基于学习的方法进行了综合比较，为后续的详细论述提供一个宏观框架。

| 方法论 | 核心原理 | 典型表示 | 主要优势 | 主要局限性 | 代表性工作 |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **信息论方法** | 最大化熵减，减少环境的概率不确定性。 | 体素占用栅格图 (Voxel Grid) | 规划过程可解释，能处理遮挡和未知空间。 | 计算成本极高（光线投射），对参数敏感，内存消耗大。 | Connolly (1985) 3, Volumetric NBV 8 |
| **几何驱动方法** | 探索已知几何的边界（如边缘、空洞）以补全模型。 | 点云、网格 (Point Cloud, Mesh) | 计算效率相对较高，直接操作表面数据。 | 无法描述未知空间，对噪声敏感，依赖启发式规则。 | Frontier-based 14, SEE 5 |
| **监督学习** | 将部分观测直接映射到最佳视点，端到端学习。 | 体素网格、点云 | 推理速度极快，无需在线搜索。 | 依赖大规模标注数据，泛化能力差，动作空间受限。 | NBV-Net 22 |
| **强化学习** | 将NBV视为序贯决策问题，通过与环境交互学习最优策略。 | 多源嵌入（几何、语义、动作） | 可处理长时程规划，支持自由空间动作，泛化能力强。 | 训练过程复杂且耗时，奖励函数设计关键，样本效率低。 | GenNBV 23, Scan-RL 24 |
| **预测引导方法** | 使用深度学习预测完整形状，再用经典规划器决策。 | 点云（部分观测和预测的完整模型） | 模块化、可解释性强，结合了学习和规划的优点。 | 性能依赖于预测模型的准确性，可能存在两阶段误差累积。 | Pred-NBV 25, MAP-NBV 26 |
| **自监督学习** | 在线自主收集数据并生成标签，实现持续学习和适应。 | 点云 | 无需离线标注数据，可适应新环境，支持终身学习。 | 依赖于高效的在线自标注机制，可能面临样本效率挑战。 | SSL-NBV 27 |

### **第3节 监督学习与自监督学习的视点预测**

#### **3.1. 监督学习：直接的端到端预测**

深度学习在NBV领域的首次尝试主要集中在监督学习范式上。

* **核心思想**：将NBV问题构建为一个监督学习任务。研究人员训练一个神经网络，通常是三维卷积神经网络（3D-CNN），来直接从一个部分观测（例如，物体的体素化表示）预测出下一个最佳视点22。  
* **优势**：这种方法在推理时非常迅速。它用一次网络前向传播取代了经典方法中耗时的搜索或优化过程，极大地提升了决策速度3。  
* **局限性**：尽管速度快，但监督学习方法存在一些严重的缺陷，这些缺陷限制了其在实际应用中的推广：  
  * **数据依赖**：它们需要大规模的数据集，其中包含大量物体以及与之对应的“最优”视点序列作为标签。生成这样的数据集既困难又昂贵22。  
  * **泛化能力差**：在一个数据集（如ShapeNet中的椅子模型）上训练的网络，往往难以泛化到不同的物体类别或与训练分布有差异的真实世界数据3。  
  * **受限的动作空间**：许多早期工作将候选的NBV限制在一个围绕物体的球面上的一组离散点，这对于许多实际的机器人任务来说是一个不切实际的约束14。  
  * **缺乏透明度**：整体式的神经网络像一个“黑箱”，其决策过程难以理解和调试，这对于需要高可靠性的应用是一个主要障碍17。

#### **3.2. 自监督学习（SSL）：在野外学习**

为了克服监督学习对大规模标注数据的依赖，自监督学习成为NBV领域一个前沿且充满希望的方向。

* **核心思想**：使机器人能够在执行任务的过程中在线学习NBV策略，通过自主生成监督信号来指导学习过程27。在这种范式下，机器人同时扮演了数据采集员和数据标注员的角色。  
* 案例研究：用于植物重建的SSL-NBV 27  
  ：  
  * **方法**：这是一个专为具有复杂几何形状的植物三维重建而设计的NBV方法。它使用一个深度网络来预测候选视点的信息增益（IG）。  
  * **自监督机制**：当机器人移动到一个选定的新视点并采集数据后，它通过比较新采集的点云与已累积的模型，来计算该视点的“真实”信息增益。这个新生成的（观测，IG）数据对随即被用作一个新的训练样本。  
  * **创新点**：该方法采用了弱监督学习（仅标注被选中的那个视点，而非所有候选视点）和经验回放（Experience Replay）机制，以实现在线高效学习。这极大地减少了对标注数据的需求（与监督学习基线相比减少了90%以上），并使模型能够适应它从未见过的新的植物类型27。  
  * **成果**：实验表明，该方法的计算速度比经典的基于体素的方法快800倍以上，并且能用比非NBV方法更少的视点完成高质量的重建27。

从监督学习到自监督学习的演变，是机器人和人工智能领域向“终身学习”和更高自主性发展的宏观趋势的一个缩影。监督学习方法遵循传统的“采集数据-训练模型-部署模型”的离线学习流程，模型一旦部署便不再变化。这种流程的关键弱点在于，当面对与训练数据分布不符的新情况时，其泛化能力会急剧下降3，而这在多变的真实世界中是常态。

相比之下，像SSL-NBV这样的自监督方法打破了这种静态模式27。机器人始终处于“学习模式”。当它遇到一个新颖的物体（例如一种新的植物）时，其性能最初可能会有所下降，但它会立即开始生成针对该物体的特定训练数据，并用这些数据在线微调其策略网络。这使得策略能够动态地适应当前环境，体现了终身学习的核心思想，为构建能在真实世界中长期可靠运行的机器人系统提供了一个更强大、更可扩展的范式。

自监督学习的成功关键在于设计一个高效且计算成本低廉的“自标注”函数。SSL-NBV的巧妙之处不仅在于其网络结构，更在于它提出了一种信息增益（IG）度量标准，这个标准可以在没有完整真值模型的情况下，仅凭机器人自身的观测数据进行实时计算。传统的IG计算方法通常需要将当前状态与一个预先存在的、完整的物体三维模型进行比较27，这对于探索未知物体来说是不可能的。SSL-NBV通过将IG重新定义为“新观测到的点云数量”与“当前已累积模型”的比率，创造了一个真实IG的有效代理指标，并且这个指标是机器人可以完全自主计算的27。这揭示了一个深刻的观点：未来自监督机器人的进步，将同样依赖于为自监督任务设计巧妙的目标函数，而不仅仅是网络结构的创新。

### **第4节 强化学习用于复杂决策**

强化学习（Reinforcement Learning, RL）为NBV问题提供了一个更强大、更灵活的框架，尤其适用于处理长时程、高维度的决策任务。

#### **4.1. 将NBV问题构建为RL问题**

RL之所以与NBV天然契合，是因为它将问题建模为一个序贯决策过程。

* **核心思想**：将NBV任务建模为一个马尔可夫决策过程（Markov Decision Process, MDP）。机器人（智能体）通过与环境的交互来学习一个策略$ \\pi(action|state) $，该策略旨在最大化在一个完整任务周期（例如，一次完整的三维重建）内的累积奖励24。这种框架天然地处理了NBV问题固有的长时程、序贯决策特性20。  
* **核心要素**：  
  * **状态（State）**：对当前知识的表示，通常包括部分重建的模型、原始传感器数据（如图像、深度图）、以及历史动作和位姿序列24。  
  * **动作（Action）**：选择下一个要前往的视点。近期研究的一个关键创新是从离散、有限的动作空间转向连续的、高自由度的自由空间动作，例如五维或六维的位姿（位置+姿态）23。  
  * **奖励（Reward）**：一个引导学习过程的标量信号。奖励函数通常基于每一步带来的重建覆盖率的提升或信息增益，并对碰撞或过长的移动路径给予惩罚24。

#### **4.2. 案例研究：GenNBV——追求泛化性**

23

* **解决的问题**：现有的NBV策略，无论是经典的还是基于学习的，都普遍缺乏泛化能力。它们依赖于人工设计的规则、针对特定场景的优化或受限的动作空间，因此在面对未见过的、复杂的几何结构时常常会失败23。  
* **方法**：GenNBV是一个端到端的、基于RL的NBV策略，专为实现跨数据集的泛化性而设计。它在一个包含多种三维场景的大规模数据集（Houses3K）上进行训练，以学习一个鲁棒的通用策略。  
* **关键创新点**：  
  1. **五维自由空间动作**：GenNBV的策略网络输出一个五维向量（三维位置，二维姿态），允许作为智能体的无人机从几乎任何视点进行扫描。这彻底打破了先前工作局限于半球或平面等受限动作空间的束缚32。  
  2. **多源状态嵌入**：为了提升泛化能力，GenNBV设计了一个信息极其丰富的状态表示。它融合了三个来源的信息：一个从概率三维栅格图中提取的**几何嵌入**，一个从RGB图像中提取的**语义嵌入**，以及一个对历史视点序列编码的**动作嵌入**31。这种表示比之前RL方法（如Scan-RL）所使用的二维表示要全面得多31。  
* **成果**：GenNBV在各项指标上显著优于基线方法。在面对未见过的物体时，它依然能达到极高的覆盖率（例如，在Houses3K测试集上达到98.26%），并且能够成功泛化到完全不同的数据集（如OmniObject3D）和非房屋类别的物体上23。

#### **4.3. 通过RL整合NBV与其他任务**

RL框架的灵活性使其能够将NBV作为一项子能力，整合到更复杂的机器人任务中。

* XPG-RL 39  
  ：这是一个为机械搜索任务（在一个杂乱场景中找到并取出目标物体）设计的框架。它将NBV策略作为一个“动作原语”与其他操纵动作（如推、拉）并列。RL策略网络学习何时切换到NBV动作，即当场景被遮挡、信息不足时，主动移动以获得更好的视角。这展示了NBV如何在一个更大的分层强化学习（Hierarchical RL, HRL）系统中作为一个可学习的子策略存在。  
* DNRSelect 41  
  ：该工作利用RL来为延迟神经渲染（Deferred Neural Rendering）选择一个最优的视点序列，将问题定义为最大化用于渲染任务的信息增益。

基于RL的NBV，特别是那些采用自由空间动作的方法，标志着该领域从**视点选择**到**轨迹优化**的转变。智能体不再是从一个预定义的列表中挑选最佳点，而是在一个高维连续空间中学习一个控制策略。经典方法和早期的监督学习方法将问题离散化：先生成候选视点，再从中选择最好的8。这里的“规划”本质上是一个选择。而像GenNBV这样的RL方法，其动作被定义为一个五维空间中的连续向量32。策略网络实际上是一个从复杂状态到这个高维动作空间的映射函数。问题不再是“这100个视点中哪个最好？”，而是“在整个空间体积内，我下一步应该去哪里？”。这从根本上讲是一个运动规划或轨迹优化问题，但其代价函数（即奖励）是通过与环境的交互隐式学习而来的，而非明确定义的。

随着策略网络（如PPO）变得越来越强大，动作空间越来越大，实现真正泛化能力的瓶颈正转移到状态表示的设计上。智能体做出明智决策的能力，越来越取决于它如何感知和内化其环境。对GenNBV的评审意见指出，其相对于Scan-RL等先前工作的主要新颖之处在于其状态表示，即使用了三维栅格而非二维地图24。GenNBV的作者也明确强调其“多源状态嵌入”是提升泛化性的关键23。这表明，仅仅拥有一个强大的RL算法是不够的。智能体泛化到一辆新汽车或一栋新房子的能力，取决于其状态表示能否提取出那些在不同实例间保持不变的抽象特征（例如，“这是一堵墙”，“那是一个被遮挡的区域”，“我之前已经去过那边了”）。因此，未来在可泛化NBV领域的突破，可能同样来自于三维场景表示学习的进步，而不仅仅是RL算法本身的发展。

### **第5节 预测引导的NBV：一种模块化的混合方法**

除了端到端的学习方法，一种强大且灵活的混合范式——预测引导的NBV——也取得了显著的成功。这种方法巧妙地结合了深度学习的感知能力和经典规划器的决策严谨性。

#### **5.1. “先预测，后规划”范式**

* **核心思想**：将NBV问题分解为两个独立的阶段。首先，利用一个深度神经网络，根据当前的部分观测来预测物体的完整三维形状。然后，将这个**预测出的完整模型**作为输入，运行一个经典的（如基于几何或信息论的）NBV规划器来决策下一个最佳视点17。  
* **优势**：这种模块化的设计解决了端到端方法的“黑箱”问题，因为其中间的预测结果是可解释的，便于调试和分析17。同时，它允许研究人员利用两个领域的最佳成果：强大的深度学习模型用于感知，以及经过充分验证、性能可靠的经典算法用于规划。

#### **5.2. 案例研究：Pred-NBV——单智能体重建**

17

* **解决的问题**：经典方法由于只能基于已知信息进行推理，效率低下。而端到端的学习方法缺乏透明度，且可能很脆弱。  
* **方法**：Pred-NBV采用一个经过改进的三维形状补全模型PoinTr-C，从一个部分点云视图预测出物体的完整点云21。然后，一个NBV规划器在这个预测出的完整模型上进行规划，通过联合优化信息增益（在  
  **预测模型**上能看到的新点数）和控制代价（移动成本）来选择下一个视点17。  
* **关键创新点**：  
  * **真实的预测**：其PoinTr-C模型经过特殊训练，能够鲁棒地处理真实世界中的情况，例如部分视图的中心与物体的真实中心不重合，这是一个常见但在许多研究中被忽略的问题17。  
  * **模块化规划**：预测模块可以与任何偏好的规划方法“即插即用”，提供了极大的灵活性21。  
* **成果**：在仿真和真实世界测试中，Pred-NBV在物体覆盖率上比传统的、非预测性的方法取得了显著提升（例如，平均提升25.46%）21。

#### **5.3. 案例研究：MAP-NBV——多智能体协作**

26

* **解决的问题**：将单智能体NBV扩展到多机器人团队并非易事。如果只是在每个机器人上独立运行NBV算法，会导致低效和冗余的行为，因为所有机器人很可能会被吸引到同一个高信息增益的区域45。  
* **方法**：MAP-NBV将预测引导的范式扩展到了多智能体系统。它是一个为实现高效协作重建而设计的去中心化算法。  
* **关键创新点**：  
  * **去中心化协调**：在通信范围内的机器人会共享它们的观测数据，以共同创建一个联合的预测模型。然后，它们通过一个序贯贪婪分配（sequential greedy assignment）的策略来分配候选视点，确保它们不会同时飞向同一个目标44。  
  * **联合信息增益**：信息增益的计算方式被重新设计，以考虑团队的联合观测。它会显式地移除那些会被多个机器人同时看到（即重叠）的点，从而避免冗余观测45。  
* **成果**：MAP-NBV相比于单智能体预测方法（提升22.75%）和非预测性的多智能体方法（提升15-19%）都取得了显著的性能改进44。

预测引导范式通过其形状补全网络，隐式地将关于世界结构的强大“先验知识”注入到规划循环中。这个网络在像ShapeNet这样的大型数据集上进行训练，学会了物体的通用结构（例如，汽车有四个轮子，飞机有两个翅膀）。当一个经典几何规划器看到汽车的部分视图时，它只能规划去往已知区域的边界。而一个预测引导的规划器看到同样的部分视图后，其预测网络会“想象”出完整的汽车，包括另一侧和车底21。此时，规划器是在这个完整的、被想象出来的汽车上进行决策。它可能会判断出信息量最大的视点在汽车的另一侧，这个决策对于一个非预测性的规划器来说是无法做出的。这种通过利用学习到的结构先验来“预见未知”17的能力，正是该范式的核心优势。

此外，这种模块化设计为持续改进和专业化分工开辟了一条清晰的道路。一个研究团队可以专注于构建更好的三维形状补全模型（这是一个通用的感知问题），而另一个团队可以专注于设计更高效的多智能体协调或规划算法。这些独立的进步可以轻松地集成在一起，而无需重新设计整个系统。例如，Pred-NBV使用了PoinTr-C作为其预测模块21。如果未来出现一个更先进的形状补全模型，Pred-NBV的开发者可以简单地替换该模块，从而立即提升系统性能。MAP-NBV的工作也明确表示它是在单智能体Pred-NBV的基础上扩展到多智能体场景的45，这清晰地展示了规划组件可以独立于感知组件进行演进。这与端到端的RL系统形成了对比，后者的感知和策略部分常常是紧密耦合的，改进其中一部分可能需要重新训练整个系统。从工程和研究效率的角度来看，这种模块化特性使得预测引导方法极具吸引力。

## **第三部分：现代前沿与未来展望**

随着三维表示方法、学习范式和任务需求的不断演进，NBV规划领域正步入一个新的发展阶段。研究的前沿正在从通用的三维重建，转向与现代三维表示方法（如NeRF和3D高斯溅射）的深度融合，以及面向特定任务和多智能体协作的专业化应用。

### **第6节 现代三维表示的NBV**

神经辐射场（NeRF）和3D高斯溅射（3D-GS）等新型表示方法的出现，正迫使NBV领域重新审视其核心目标和方法论。

#### **6.1. 神经辐射场（NeRF）的主动规划**

对于NeRF而言，NBV的目标不再仅仅是追求几何上的完整性，而是最大化新视角合成的质量，通常以峰值信噪比（PSNR）、结构相似性指数（SSIM）和学习感知图像块相似度（LPIPS）等指标来衡量15。研究表明，简单地最大化几何覆盖率并不能保证得到一个更高质量的NeRF模型11。

* **基于不确定性的方法**：一些研究尝试在NeRF模型内部对不确定性进行建模。例如，ActiveNeRF将辐射场中的辐射值建模为高斯分布，并选择能够最大程度降低方差的视点48。另一些方法则利用费雪信息（Fisher Information）作为信息增益的代理指标16。然而，这些方法通常需要修改NeRF的底层架构，并且计算成本高昂48。  
* 代理目标（SOAR）15  
  ：这是一种“指令性”（prescriptive）方法，它在**不**需要预先训练任何NeRF模型的情况下，定义了何为对NeRF有益的视点。  
  * **方法**：SOAR提出了一套可解释的代理目标函数来评估视点的优劣，包括：表面覆盖率、几何复杂度、纹理复杂度和光线多样性。研究者训练了一个名为SOARNet的轻量级网络来快速预测这些分数，从而将视点选择的时间从数小时缩短到数秒15。  
  * **创新点**：这种方法与具体的辐射场模型无关（model-agnostic），意味着同一个视点选择策略可以应用于不同的辐射场模型，如原始NeRF、Instant-NGP等15。

#### **6.2. 3D高斯溅射（3D-GS）的信息论规划**

与NeRF的隐式表示不同，3D-GS是一种显式表示，这为应用更经典的信息论分析方法打开了大门。

* **挑战**：标准的3D-GS模型本身不提供不确定性的量化，而这对于主动感知是必不可少的52。  
* 案例研究：POp-GS 51  
  ：  
  * **核心思想**：通过最优实验设计（Optimal Experimental Design）这一经典统计学框架，重新构建3D-GS的信息增益问题。  
  * **方法**：该工作通过计算3D高斯参数的费雪信息矩阵来构建协方差矩阵，从而量化模型的不确定性。然后，它应用P-最优性（P-Optimality）准则来选择NBV。  
  * **P-最优性准则**：  
    * **T-最优性（T-Optimality）**：旨在最小化协方差矩阵的迹（trace），代表最小化参数的平均不确定性。  
    * **D-最优性（D-Optimality）**：旨在最小化协方差矩阵的行列式（determinant），代表最小化不确定性椭球的体积。  
  * **成果**：实验证明，D-最优性和T-最优性是选择能够显著提升3D-GS重建质量的视点的有效度量，其性能优于其他基线方法53。

从传统的几何表示（网格、体素）到现代的神经表示（NeRF、3D-GS）的转变，引发了“信息”定义上的一次根本性分裂。信息不再仅仅关乎几何（覆盖率），而是与**光度学**和**模型的可学习性**紧密相连。一个“好”的视点，不仅要能看到新东西，更要以一种对底层模型的优化过程最有利的方式去看。对于一个网格模型，如果一个新视点能增加新的三角面片，它就是好的。但对于NeRF，一个好视点需要帮助其底层的多层感知机（MLP）更好地学习从空间位姿到颜色和密度的映射。而能够帮助MLP学习的，正是那些具有高纹理复杂度、多样化光线角度以及能够解决几何模糊性的视点——这恰恰是SOAR提出的代理目标15。这表明，NBV问题正变得与底层表示的“学习动态”相耦合：我们不再是为机器人规划，而是在为神经网络规划。

与此同时，NBV研究领域呈现出一种周期性趋势。该领域从经典的、基于模型的信息论（用于体素）出发，转向了无模型的、端到端的学习方法。而现在，随着像3D-GS这样的显式表示的出现，我们看到信息论的回归——将最优实验设计的经典理论应用于一个更复杂、更具表现力的现代模型（如POp-GS）53。这表明，随着新的显式表示方法的不断涌现，经典的、基于第一性原理的信息论规划技术将迎来复兴。

### **第7节 面向任务与协作的NBV**

NBV研究的另一个重要前沿是超越“为重建而重建”的范畴，将视点规划策略与具体的下游任务和多智能体协作场景深度结合。

#### **7.1. 特定应用的视点规划**

这种研究趋势旨在使NBV策略服务于特定的最终目标，从而大幅提升任务效率。

* 面向抓取的NBV（ACE-NBV）6  
  ：  
  * **目标**：为一个被遮挡的物体找到一个可行的抓取姿态，而非完整地重建它。  
  * **方法**：利用一个隐式神经表示来“想象”在未见过的视点下的抓取可能性。NBV的选择基于哪个视点被预测将带来最高的抓取质量。该策略的核心直觉是，测量一个抓取姿态的最佳视角，通常与抓取方向本身是对齐的6。  
* 面向分类的NBV 30  
  ：  
  * **目标**：从一个已知的物体集合中识别出当前物体。  
  * **方法**：使用深度强化学习（Soft Actor-Critic）来学习一个策略，该策略会控制机械臂将持有的物体旋转到一个能为分类网络提供最具区分性信息的姿态30。  
* 面向巡检的NBV 4  
  ：  
  * **目标**：在考虑障碍物的同时，用最少的、信息最丰富的照片完成巡检任务，同时优化照片的透视畸变、目标尺度和覆盖范围。  
  * **方法**：使用高斯过程插值和无导数优化方法，根据一个专门为巡检任务设计的评估指标来寻找最佳视点4。

#### **7.2. 多智能体协作NBV**

如何高效地利用机器人团队进行主动感知是一个复杂而重要的问题。

* **核心挑战**：协调是关键。必须避免多个智能体对同一区域进行冗余观测，从而浪费资源45。  
* 预测引导的协调（MAP-NBV）26  
  ：如第5.3节所述，该方法通过共享预测模型和去中心化的贪婪分配策略来实现高效协作。  
* 基于子模性（Submodularity）的协调 58  
  ：这是一种更具理论性的方法，它将多机器人NBV问题构建为最大化一个子模函数的过程。研究人员设计了一个能够感知重叠的效用函数，并证明了该函数是子模且单调递增的。这一性质保证了通过一个简单的贪婪算法就能找到一个有界近似的最优解（在最优解的1/2范围内）58。这为设计高效、去中心化的协调策略提供了坚实的理论基础。

面向任务的NBV的兴起，标志着该领域的成熟。它正从一个通用的“数据采集”问题，转变为一个专业的“决策支持”问题。机器人不再问“我如何才能看到所有东西？”，而是问“为了完成我的特定目标，我最少需要看到什么？”。这种从以数据为中心到以目标为中心的主动感知的转变，极大地提升了机器人在实际应用中的效率。例如，对于抓取任务，机器人无需看到物体的背面，ACE-NBV 6 正式化了这一点，一旦找到一个可行的抓取方案就停止探索，这远比完整重建要高效。

在多智能体协作方面，挑战正从纯粹的信息最大化转向通信受限的、去中心化的优化。最高效的未来系统不会假设所有智能体在任何时候都能相互通信，而是会开发出能够处理间歇性或有限通信的鲁棒策略。MAP-NBV假设在一个“通信子图”内的机器人可以共享观测44，这是一个较强的假设。其协调效果的实验也明确比较了中心化、去中心化和非协作方法，展示了其中的权衡44。在现实世界的机器人团队任务中（如搜救），必然会面临带宽限制和网络分区。真正的挑战不仅在于协调，更在于在这些约束下进行

**鲁棒**的协调。这预示着未来的研究需要整合来自网络控制系统和信息论的概念，以开发出能够根据通信质量优雅地降级其协调策略的NBV算法，例如在通信中断时退化为更具个体主义的行为。

### **第8节 挑战综合与未来研究轨迹**

经过数十年的发展，NBV规划领域虽然取得了长足的进步，但仍面临诸多挑战。对这些挑战的深刻理解，为未来的研究指明了方向。

#### **8.1. 持续存在的挑战**

* **计算复杂度**：尽管算法不断优化，但确定最优视点仍然是一个计算密集型任务，尤其对于需要在资源受限的机器人上进行实时规划的应用而言2。经典信息论方法中的光线投射瓶颈问题至今仍是衡量新方法效率的一个基准3。  
* **环境与传感器不确定性**：真实世界是杂乱、动态的，且传感器数据充满噪声2。算法必须对这些不完美性具有鲁棒性，包括传感器的定位误差8。  
* **泛化性与“模拟到现实”的鸿沟**：基于学习的方法虽然强大，但仍难以将从训练数据（通常是仿真环境）中学到的知识泛化到新颖的、未见过的真实世界物体和场景中3。这是阻碍其广泛部署的一个关键障碍38。  
* **可解释性与信任**：许多现代深度学习策略如同“黑箱”，其决策过程难以解释、调试或信任。这在安全关键型应用中是一个重大隐患2。  
* **高维动作空间**：在六自由度（6-DOF）空间中直接找到最优视点是一个极具挑战性的优化问题，这迫使许多方法依赖于对候选视点的采样，而采样本身可能无法覆盖最优解3。

#### **8.2. 未来研究方向**

* **终身与持续学习**：在SSL-NBV等工作的成功基础上27，开发能够在漫长的运行生命周期中，在不断变化的环境里持续学习和调整其感知与规划策略的智能体，是一个重要的前沿方向。  
* **任务与语义驱动的NBV**：超越重建本身，更深层次地整合语义理解。未来的规划器应该能够推理物体的类别、属性及其功能（affordances），并以“这是什么物体？我能对它做什么？”等问题来引导视点规划，正如ACE-NBV所展示的初步探索6。  
* **多智能体与人机协作**：开发更先进的多智能体协调策略，使其能够鲁棒地应对通信限制45。此外，实现人机协同感知，让NBV规划器能够融合人类用户的偏好或目标，是使机器人成为更有效助手的关键研究领域2。  
* **非结构化与基于点的表示**：继续探索像SEE 5 那样直接在非结构化表示（如点云）上操作的方法，以避免体素化带来的信息损失，并能更好地处理精细细节。  
* **隐式-显式混合规划**：结合新型表示的优点。例如，在同一个规划循环中，利用3D高斯溅射进行快速、显式的几何推理，同时利用NeRF进行高质量的纹理分析和渲染。  
* **端到端可微规划器**：一个长远的目标是开发完全可微分的规划系统。在这种系统中，来自最终任务的损失（例如，抓取失败）的梯度可以反向传播通过整个感知和规划流水线，从而直接优化视点选择策略。这将是实现真正意义上的端到端任务驱动主动感知的终极形态。

#### **引用的著作**

1. Optimizing Robotics with Next-Best-View Planning Strategies \- Number Analytics, 访问时间为 六月 27, 2025， [https://www.numberanalytics.com/blog/optimizing-robotics-next-best-view-planning](https://www.numberanalytics.com/blog/optimizing-robotics-next-best-view-planning)  
2. Mastering Next-Best-View Planning \- Number Analytics, 访问时间为 六月 27, 2025， [https://www.numberanalytics.com/blog/mastering-next-best-view-planning](https://www.numberanalytics.com/blog/mastering-next-best-view-planning)  
3. PB-NBV: Efficient Projection-Based Next-Best-View Planning Framework for Reconstruction of Unknown Objects \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2501.10663v1](https://arxiv.org/html/2501.10663v1)  
4. Sampling-Based Next-Best-View Planning for Autonomous Photography & Inspection \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2403.05477v1](https://arxiv.org/html/2403.05477v1)  
5. Next Best View Planning with an Unstructured Representation \- ESP, 访问时间为 六月 27, 2025， [https://robotic-esp.com/papers/border\_dphil19.pdf](https://robotic-esp.com/papers/border_dphil19.pdf)  
6. Affordance-Driven Next-Best-View Planning for Robotic Grasping \- Proceedings of Machine Learning Research, 访问时间为 六月 27, 2025， [https://proceedings.mlr.press/v229/zhang23i/zhang23i.pdf](https://proceedings.mlr.press/v229/zhang23i/zhang23i.pdf)  
7. Mastering Next-Best-View Planning in Robotics \- Number Analytics, 访问时间为 六月 27, 2025， [https://www.numberanalytics.com/blog/next-best-view-planning-robotics-ultimate-guide](https://www.numberanalytics.com/blog/next-best-view-planning-robotics-ultimate-guide)  
8. Volumetric Next-Best-View Planning for 3D Object Reconstruction with Positioning Error, 访问时间为 六月 27, 2025， [https://www.researchgate.net/publication/266548624\_Volumetric\_Next-Best-View\_Planning\_for\_3D\_Object\_Reconstruction\_with\_Positioning\_Error](https://www.researchgate.net/publication/266548624_Volumetric_Next-Best-View_Planning_for_3D_Object_Reconstruction_with_Positioning_Error)  
9. Next best view planning for active model improvement \- BMVA Archive, 访问时间为 六月 27, 2025， [https://bmva-archive.org.uk/bmvc/2009/Papers/Paper436/Paper436.pdf](https://bmva-archive.org.uk/bmvc/2009/Papers/Paper436/Paper436.pdf)  
10. Next best view planning: Conceptual overview. The approach has two steps \- ResearchGate, 访问时间为 六月 27, 2025， [https://www.researchgate.net/figure/Next-best-view-planning-Conceptual-overview-The-approach-has-two-steps-A-an\_fig5\_313680980](https://www.researchgate.net/figure/Next-best-view-planning-Conceptual-overview-The-approach-has-two-steps-A-an_fig5_313680980)  
11. \[2505.06219\] VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/abs/2505.06219](https://arxiv.org/abs/2505.06219)  
12. Guided Next Best View for 3D Reconstruction of Large Complex Structures \- MDPI, 访问时间为 六月 27, 2025， [https://www.mdpi.com/2072-4292/11/20/2440](https://www.mdpi.com/2072-4292/11/20/2440)  
13. View planning in robot active vision: A survey of systems, algorithms, and applications \- SciOpen, 访问时间为 六月 27, 2025， [https://www.sciopen.com/article\_pdf/1397761204106649601.pdf](https://www.sciopen.com/article_pdf/1397761204106649601.pdf)  
14. Pred-NBV: Prediction-guided Next-Best-View Planning for 3D Object Reconstruction \- GitHub Pages, 访问时间为 六月 27, 2025， [https://mit-spark.github.io/robotRepresentations-RSS2023/assets/papers/3.pdf](https://mit-spark.github.io/robotRepresentations-RSS2023/assets/papers/3.pdf)  
15. arxiv.org, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2312.03266v1](https://arxiv.org/html/2312.03266v1)  
16. Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2506.19844v1](https://arxiv.org/html/2506.19844v1)  
17. Pred-NBV: Prediction-guided Next-Best-View Planning for 3D Object Reconstruction, 访问时间为 六月 27, 2025， [https://par.nsf.gov/servlets/purl/10489772](https://par.nsf.gov/servlets/purl/10489772)  
18. (PDF) Next-Best-View Planning for 3D Object Reconstruction under Positioning Error, 访问时间为 六月 27, 2025， [https://www.researchgate.net/publication/220887545\_Next-Best-View\_Planning\_for\_3D\_Object\_Reconstruction\_under\_Positioning\_Error](https://www.researchgate.net/publication/220887545_Next-Best-View_Planning_for_3D_Object_Reconstruction_under_Positioning_Error)  
19. (PDF) Assessment of next-best-view algorithms performance with various 3D scanners and manipulator \- ResearchGate, 访问时间为 六月 27, 2025， [https://www.researchgate.net/publication/305141853\_Assessment\_of\_next-best-view\_algorithms\_performance\_with\_various\_3D\_scanners\_and\_manipulator](https://www.researchgate.net/publication/305141853_Assessment_of_next-best-view_algorithms_performance_with_various_3D_scanners_and_manipulator)  
20. Where to Look Next: Learning Viewpoint Recommendations for Informative Trajectory Planning \- Autonomous Multi-Robots Lab, 访问时间为 六月 27, 2025， [https://autonomousrobots.nl/assets/files/publications/22-lodel-icra.pdf](https://autonomousrobots.nl/assets/files/publications/22-lodel-icra.pdf)  
21. Pred-NBV: Prediction-guided Next-Best-View Planning for 3D Object Reconstruction \- RAAS Lab, 访问时间为 六月 27, 2025， [https://raaslab.org/pubs/dhami2023prednbv.pdf](https://raaslab.org/pubs/dhami2023prednbv.pdf)  
22. Supervised Learning of the Next-Best-View for 3D Object Reconstruction \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/abs/1905.05833](https://arxiv.org/abs/1905.05833)  
23. GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/abs/2402.16174](https://arxiv.org/abs/2402.16174)  
24. GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction \- OpenReview, 访问时间为 六月 27, 2025， [https://openreview.net/forum?id=3OtVLnXfmS](https://openreview.net/forum?id=3OtVLnXfmS)  
25. Pred-NBV: Prediction-guided Next-Best-View for 3D Object Reconstruction \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/abs/2304.11465](https://arxiv.org/abs/2304.11465)  
26. arxiv.org, 访问时间为 六月 27, 2025， [https://arxiv.org/abs/2307.04004](https://arxiv.org/abs/2307.04004)  
27. SSL-NBV: A Self-Supervised-Learning-Based Next-Best ... \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/pdf/2410.14790](https://arxiv.org/pdf/2410.14790)  
28. MACARONS: Mapping And Coverage Anticipation with RGB Online Self-Supervision \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/abs/2303.03315](https://arxiv.org/abs/2303.03315)  
29. SSL-NBV: A Self-Supervised-Learning-Based Next-Best-View algorithm for Efficient 3D Plant Reconstruction by a Robot | Request PDF \- ResearchGate, 访问时间为 六月 27, 2025， [https://www.researchgate.net/publication/384710900\_Ssl-Nbv\_A\_Self-Supervised-Learning-Based\_Next-Best-View\_Algorithm\_for\_Efficient\_3d\_Plant\_Reconstruction\_by\_a\_Robot](https://www.researchgate.net/publication/384710900_Ssl-Nbv_A_Self-Supervised-Learning-Based_Next-Best-View_Algorithm_for_Efficient_3d_Plant_Reconstruction_by_a_Robot)  
30. \[2110.06766\] Next-Best-View Estimation based on Deep Reinforcement Learning for Active Object Classification \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/abs/2110.06766](https://arxiv.org/abs/2110.06766)  
31. arxiv.org, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2402.16174v2](https://arxiv.org/html/2402.16174v2)  
32. GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2402.16174v3](https://arxiv.org/html/2402.16174v3)  
33. \[Literature Review\] GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction \- Moonlight | AI Colleague for Research Papers, 访问时间为 六月 27, 2025， [https://www.themoonlight.io/en/review/gennbv-generalizable-next-best-view-policy-for-active-3d-reconstruction](https://www.themoonlight.io/en/review/gennbv-generalizable-next-best-view-policy-for-active-3d-reconstruction)  
34. GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2402.16174v1](https://arxiv.org/html/2402.16174v1)  
35. GenNBV: Generalizable Next-Best-View Policy for Active 3D ..., 访问时间为 六月 27, 2025， [https://gennbv.tech/](https://gennbv.tech/)  
36. GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction, 访问时间为 六月 27, 2025， [https://openaccess.thecvf.com/content/CVPR2024/papers/Chen\_GenNBV\_Generalizable\_Next-Best-View\_Policy\_for\_Active\_3D\_Reconstruction\_CVPR\_2024\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_GenNBV_Generalizable_Next-Best-View_Policy_for_Active_3D_Reconstruction_CVPR_2024_paper.pdf)  
37. GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction \- ResearchGate, 访问时间为 六月 27, 2025， [https://www.researchgate.net/publication/383240723\_GenNBV\_Generalizable\_Next-Best-View\_Policy\_for\_Active\_3D\_Reconstruction](https://www.researchgate.net/publication/383240723_GenNBV_Generalizable_Next-Best-View_Policy_for_Active_3D_Reconstruction)  
38. GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction \- AIModels.fyi, 访问时间为 六月 27, 2025， [https://www.aimodels.fyi/papers/arxiv/gennbv-generalizable-next-best-view-policy-active](https://www.aimodels.fyi/papers/arxiv/gennbv-generalizable-next-best-view-policy-active)  
39. XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search \- arXiv, 访问时间为 六月 27, 2025， [http://arxiv.org/pdf/2504.20969](http://arxiv.org/pdf/2504.20969)  
40. XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2504.20969v1](https://arxiv.org/html/2504.20969v1)  
41. DNRSelect: Active Best View Selection for Deferred Neural Rendering \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/pdf/2501.12150?](https://arxiv.org/pdf/2501.12150)  
42. Pred-NBV: Prediction-guided Next-Best-View for 3D Object Reconstruction \- RAAS Lab, 访问时间为 六月 27, 2025， [https://raaslab.org/projects/PredNBV/](https://raaslab.org/projects/PredNBV/)  
43. Pred-NBV: Prediction-guided Next-Best-View for 3D Object Reconstruction \- ResearchGate, 访问时间为 六月 27, 2025， [https://www.researchgate.net/publication/370227498\_Pred-NBV\_Prediction-guided\_Next-Best-View\_for\_3D\_Object\_Reconstruction](https://www.researchgate.net/publication/370227498_Pred-NBV_Prediction-guided_Next-Best-View_for_3D_Object_Reconstruction)  
44. MAP-NBV: Multi-agent Prediction-guided Next-Best-View Planning for Active 3D Object Reconstruction \- RAAS Lab, 访问时间为 六月 27, 2025， [http://raaslab.org/projects/MAPNBV/](http://raaslab.org/projects/MAPNBV/)  
45. MAP-NBV: Multi-agent Prediction-guided Next-Best-View Planning for Active 3D Object Reconstruction \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2307.04004v3](https://arxiv.org/html/2307.04004v3)  
46. MAP-NBV: Multi-agent Prediction-guided Next-Best-View Planning for Active 3D Object Reconstruction \- ResearchGate, 访问时间为 六月 27, 2025， [https://www.researchgate.net/publication/372248187\_MAP-NBV\_Multi-agent\_Prediction-guided\_Next-Best-View\_Planning\_for\_Active\_3D\_Object\_Reconstruction](https://www.researchgate.net/publication/372248187_MAP-NBV_Multi-agent_Prediction-guided_Next-Best-View_Planning_for_Active_3D_Object_Reconstruction)  
47. arxiv.org, 访问时间为 六月 27, 2025， [https://arxiv.org/pdf/2307.04004](https://arxiv.org/pdf/2307.04004)  
48. ConMax3D: Frame Selection for 3D Reconstruction Through Concept Maximization \- SciTePress, 访问时间为 六月 27, 2025， [https://www.scitepress.org/Papers/2025/132588/132588.pdf](https://www.scitepress.org/Papers/2025/132588/132588.pdf)  
49. Active Perception using Neural Radiance Fields \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/html/2310.09892v2](https://arxiv.org/html/2310.09892v2)  
50. \[2312.03266\] SO-NeRF: Active View Planning for NeRF using Surrogate Objectives \- arXiv, 访问时间为 六月 27, 2025， [https://arxiv.org/abs/2312.03266](https://arxiv.org/abs/2312.03266)  
51. arxiv.org, 访问时间为 六月 27, 2025， [https://arxiv.org/pdf/2503.07819](https://arxiv.org/pdf/2503.07819)  
52. POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality, 访问时间为 六月 27, 2025， [https://openaccess.thecvf.com/content/CVPR2025/papers/Wilson\_POp-GS\_Next\_Best\_View\_in\_3D-Gaussian\_Splatting\_with\_P-Optimality\_CVPR\_2025\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Wilson_POp-GS_Next_Best_View_in_3D-Gaussian_Splatting_with_P-Optimality_CVPR_2025_paper.pdf)  
53. POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality \- CVPR 2025, 访问时间为 六月 27, 2025， [https://cvpr.thecvf.com/virtual/2025/poster/34708](https://cvpr.thecvf.com/virtual/2025/poster/34708)  
54. POp-GS: Next best view in 3D-Gaussian splatting with P-Optimality, 访问时间为 六月 27, 2025， [https://www.amazon.science/publications/pop-gs-next-best-view-in-3d-gaussian-splatting-with-p-optimality](https://www.amazon.science/publications/pop-gs-next-best-view-in-3d-gaussian-splatting-with-p-optimality)  
55. arxiv.org, 访问时间为 六月 27, 2025， [https://arxiv.org/abs/2503.07819](https://arxiv.org/abs/2503.07819)  
56. \[Literature Review\] POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality, 访问时间为 六月 27, 2025， [https://www.themoonlight.io/en/review/pop-gs-next-best-view-in-3d-gaussian-splatting-with-p-optimality](https://www.themoonlight.io/en/review/pop-gs-next-best-view-in-3d-gaussian-splatting-with-p-optimality)  
57. Affordance-Driven Next-Best-View Planning for Robotic Grasping | OpenReview, 访问时间为 六月 27, 2025， [https://openreview.net/forum?id=IeKC9khX5jD](https://openreview.net/forum?id=IeKC9khX5jD)  
58. Multi-robot next-best-view planning | Mikko Lauri, 访问时间为 六月 27, 2025， [https://laurimi.github.io/research/2020/07/04/multirobot-nbv.html](https://laurimi.github.io/research/2020/07/04/multirobot-nbv.html)