# 讲义：从 VAE 到 VQ-VAE：理解生成模型中的连续与离散表征

**目标：** 本讲义旨在解释变分自编码器 (VAE) 的基本原理，探讨在 VAE 框架中使用离散隐变量时遇到的挑战，并详细介绍向量量化 VAE (VQ-VAE) 是如何巧妙地解决这些挑战，从而学习到有意义的离散数据表征。

**结构：**

1.  **引言：生成模型与表示学习**
2.  **Part 1: 标准变分自编码器 (Continuous VAE)**
    *   目标与架构
    *   核心困难：难解的$p(x)$
    *   解决方案：变分推断与 ELBO
    *   关键技巧：重参数化 (Reparameterization Trick)
    *   梯度顺畅流动
3.  **Part 2: 离散隐变量的挑战**
    *   动机：为何需要离散表征？
    *   核心问题：采样操作的不可微性
    *   **简单示例：理解梯度流动阻塞**
    *   尝试的解决方案及其局限性 (REINFORCE, Gumbel-Softmax)
    *   后验坍塌 (Posterior Collapse) 问题
4.  **Part 3: VQ-VAE：一种成功的离散 VAE**
    *   核心思想：向量量化
    *   架构与机制
    *   解决梯度问题：直通估计器 (STE)
    *   独特的损失函数
    *   避免后验坍塌
    *   两阶段训练范式
5.  **Part 4: 对比总结**
6.  **结论**

---

### 1. 引言：生成模型与表示学习

在机器学习中，**无监督学习**是一个核心挑战，其目标是在没有标签的情况下从数据中发现有用的结构或模式。**生成模型 (Generative Models)** 是无监督学习的一个重要分支，它们的目标是学习数据的底层分布$p(x)$，从而能够生成与训练数据相似的新样本。

**表示学习 (Representation Learning)** 则关注于学习数据的有效表示，这些表示能够捕捉数据的关键特征，便于后续任务（如分类、回归、聚类）。理想的表示应该是紧凑的、有意义的、并且能够分离出数据的变异因素。

**变分自编码器 (Variational Autoencoder, VAE)** [Kingma & Welling, 2013] 是一个强大的生成模型框架，它巧妙地结合了深度学习和概率图模型的思想，既能生成数据，又能学习到数据的低维潜在表示 (latent representation)。

---

### Part 1: 标准变分自编码器 (Continuous VAE)

#### 目标与架构

*   **目标:** 学习数据$x$的真实分布$p(x)$。VAE 通过引入一个低维的**连续隐变量 (latent variable)$z$** 来实现这一点。它假设数据$x$是由$z$生成的，即$x \sim p_\theta(x|z)$，而$z$本身服从一个简单的先验分布$p(z)$（通常是标准正态分布$N(0, I)$）。
*   **架构:** VAE 主要由两部分组成：
    *   **编码器 (Encoder) / 推断网络$q_\phi(z|x)$:** 输入数据$x$，输出隐变量$z$的后验分布的参数。对于高斯 VAE，它输出均值$\mu_\phi(x)$和方差$\sigma^2_\phi(x)$。这个网络的作用是从数据推断出可能的隐变量表示。
    *   **解码器 (Decoder) / 生成网络$p_\theta(x|z)$:** 输入隐变量$z$，输出原始数据$x$的分布参数（例如，如果是图像，可以输出每个像素的均值，或者伯努利分布的概率）。这个网络的作用是从隐变量重构数据。

#### 核心困难：难解的$p(x)$

根据模型假设，$p(x) = \int p_\theta(x|z) p(z) dz$。这个积分通常是**难解 (intractable)** 的，因为它需要在整个高维$z$空间上进行积分，计算复杂度极高。

#### 解决方案：变分推断与 ELBO

VAE 使用**变分推断 (Variational Inference)** 的思想来绕过直接计算$p(x)$。它引入编码器$q_\phi(z|x)$来近似真实的后验分布$p_\theta(z|x) = p_\theta(x|z)p(z)/p(x)$。

通过数学推导，可以得到$p(x)$的一个**证据下界 (Evidence Lower BOund, ELBO)**：
$\log p(x) \ge \mathcal{L}(\theta, \phi; x) = E_{z \sim q_\phi(z|x)} [\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z))$

最大化 ELBO$\mathcal{L}(\theta, \phi; x)$就等价于：

1.  **最小化近似后验$q_\phi(z|x)$和真实后验$p_\theta(z|x)$之间的 KL 散度。** (让我们的近似更接近真实后验)
2.  **最大化数据的对数似然$\log p(x)$的下界。** (让模型更好地拟合数据)

ELBO 包含两项：

*   **重建项 (Reconstruction Term):**$E_{z \sim q_\phi(z|x)} [\log p_\theta(x|z)]$
    *   意义：衡量在给定从编码器得到的隐变量$z$后，解码器重建原始输入$x$的好坏程度。期望编码器产生的$z$包含足够的信息来重构$x$。
*   **KL 散度项 (KL Divergence Term):**$D_{KL}(q_\phi(z|x) || p(z))$
    *   意义：一个正则项，衡量编码器产生的后验分布$q_\phi(z|x)$与我们设定的先验分布$p(z)$(如$N(0, I)$) 之间的距离。期望编码器产生的$z$的分布不会离先验太远，使得隐空间具有良好的结构（例如，连续、完整）。

#### 关键技巧：重参数化 (Reparameterization Trick)

为了使用梯度下降优化 ELBO，我们需要计算 ELBO 关于参数$\theta$和$\phi$的梯度。主要的困难在于计算重建项$E_{z \sim q_\phi(z|x)} [\log p_\theta(x|z)]$关于编码器参数$\phi$的梯度$\nabla_\phi E_{z \sim q_\phi(z|x)} [\dots]$。

问题在于期望是关于$q_\phi(z|x)$的，而$q_\phi(z|x)$本身就依赖于$\phi$。直接从$q_\phi(z|x)$中采样$z$是一个随机步骤，这使得梯度无法通过采样操作回传给$\phi$。

**重参数化技巧**解决了这个问题：
假设$q_\phi(z|x)$是高斯分布$N(\mu_\phi(x), \sigma^2_\phi(x))$。我们可以不直接从这个分布采样$z$，而是：
1.  从一个固定的、与$\phi$无关的标准分布中采样一个噪声$\epsilon$，例如$\epsilon \sim N(0, I)$。
2.  通过一个确定性变换计算$z$：$z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon$。

**为什么这有效？**

*   **分布等价:** 这样生成的$z$仍然服从$N(\mu_\phi(x), \sigma^2_\phi(x))$分布。
*   **分离随机性:** 随机性现在完全来自于与$\phi$无关的$\epsilon$。
*   **确定性路径:** 对于一个固定的$\epsilon$，从$\phi$到$\mu_\phi, \sigma_\phi$，再到$z$，再到$\log p_\theta(x|z)$的整个计算路径是**确定且可微的**。

#### 梯度顺畅流动

使用重参数化后，重建项的梯度可以写成：
$\nabla_\phi E_{z \sim q_\phi(z|x)} [\log p_\theta(x|z)] = \nabla_\phi E_{\epsilon \sim N(0, I)} [\log p_\theta(x|z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon)]$
由于期望是关于固定的$p(\epsilon)$，我们可以把梯度$\nabla_\phi$移入期望内部：
$= E_{\epsilon \sim N(0, I)} [\nabla_\phi \log p_\theta(x|z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon)]$
现在，期望内部的梯度可以通过标准的反向传播（链式法则）轻松计算。梯度可以从损失$\log p_\theta(x|z)$顺畅地流回解码器参数$\theta$（通过$\nabla_z \log p_\theta(x|z)$）和编码器参数$\phi$（通过$\nabla_\phi z = \nabla_\phi (\mu_\phi + \sigma_\phi \epsilon)$）。

---

### Part 2: 离散隐变量的挑战

#### 动机：为何需要离散表征？

虽然连续隐变量 VAE 很成功，但在某些场景下，我们可能更希望使用**离散隐变量$z$**：

*   **数据本质:** 某些数据类型（如语言是词汇的序列）本身就具有离散结构。
*   **可解释性:** 离散状态有时比连续空间中的一个点更容易解释。
*   **下游任务:** 某些推理或规划任务可能更适合在离散空间中进行。
*   **信息压缩:** 离散变量可以实现更强的、有损的信息压缩。

#### 核心问题：采样操作的不可微性

当$z$是离散的时（例如，从$K$个类别中选择一个，即$z \sim Categorical(\pi_\phi(x))$），**重参数化技巧通常不再适用**。从分类分布中采样的操作是一个**不可微**的过程。

#### **简单示例：理解梯度流动阻塞**

让我们用一个简单例子来说明：
假设$\theta$是参数，$p = \text{sigmoid}(\theta)$是概率，$z$是输出，$L = (z-\text{target})^2$是损失。

*   **连续可微情况 (模拟重参数化):** 如果$z = 2p$，那么$\theta \rightarrow p \rightarrow z \rightarrow L$的每一步都是可微的。梯度$\nabla_\theta L = \frac{dL}{dz} \cdot \frac{dz}{dp} \cdot \frac{dp}{d\theta}$可以顺畅计算。
*   **离散采样情况:** 如果$z \sim Bernoulli(p)$，那么从$p$到$z$的**采样步骤是不可微的**。改变$p$不会平滑地改变$z$，而是会导致$z$在 0 和 1 之间**跳变**。因此，导数$\frac{dz}{dp}$没有良好定义。标准的链式法则在这里中断，梯度无法从$L$流回$p$和$\theta$。

这就是离散 VAE 面临的核心梯度问题。

#### 尝试的解决方案及其局限性

1.  **得分函数估计器 (Score Function / REINFORCE):**
    *   **原理:** 利用 log-derivative trick:$\nabla_\phi E_{q_\phi(z|x)}[f(z)] = E_{q_\phi(z|x)}[f(z) \nabla_\phi \log q_\phi(z|x)]$。它不需要$f(z)$可微，只需要$\log q_\phi(z|x)$关于$\phi$可微（我们之前讨论过，这通常是成立的）。
    *   **应用:**$\nabla_\phi E_{q_\phi(z|x)}[\log p_\theta(x|z)] \approx \log p_\theta(x|z) \cdot \nabla_\phi \log q_\phi(z|x)$（使用单一样本$z$估计）。
    *   **缺点:** 这个估计器的**方差极高**。单一样本的梯度估计可能非常嘈杂，导致训练非常不稳定，收敛缓慢。需要大量样本或复杂的方差缩减技术。

2.  **连续松弛 (Continuous Relaxation / Gumbel-Softmax):**
    *   **原理:** 用一个可微的连续分布（如 Gumbel-Softmax 分布）来近似离散的 Categorical 分布。这个连续分布有一个“温度”参数$\tau$。
    *   **优点:** 使得重参数化技巧可以再次被应用，梯度可以通过反向传播计算。
    *   **缺点:**
        *   **有偏梯度:** 近似引入了偏差，尤其在温度$\tau$较高时。
        *   **温度退火:** 需要仔细调整温度$\tau$从高到低的退火策略。
        *   **低温不稳定:** 当$\tau$接近 0 时，分布接近离散，但梯度可能变得非常大或不稳定。
        *   **实践困难:** VQ-VAE 论文提到，他们发现用 Gumbel-Softmax 从头训练效果不佳，解码器可能学会利用连续松弛的特性。

#### 后验坍塌 (Posterior Collapse) 问题

这个问题在离散 VAE 中尤其严重。

*   **现象:** KL 散度项$D_{KL}(q_\phi(z|x) || p(z))$在优化过程中很快趋于 0。这意味着编码器产生的后验$q_\phi(z|x)$对于所有输入$x$都变得和先验$p(z)$没有区别。
*   **后果:** 隐变量$z$没有学到任何关于输入$x$的有用信息，编码器被“忽略”了。模型退化了，失去了学习有意义表征的能力。
*   **原因:**
    *   **强大的解码器:** 如果解码器$p_\theta(x|z)$本身能力很强（如自回归模型），它可以不依赖$z$就很好地建模$p(x)$或$p(x|z)$。
    *   **ELBO 优化捷径:** 模型为了最大化 ELBO，发现将 KL 项降为 0 是一个“容易”的途径，尤其是当梯度信号弱（由于离散 VAE 的梯度问题）或解码器不需要$z$时。

---

### Part 3: VQ-VAE：一种成功的离散 VAE

VQ-VAE [van den Oord et al., 2017] 提出了一种创新的方法来学习离散表征，有效解决了上述问题。

#### 核心思想：向量量化

VQ-VAE 的核心不是直接对概率分布采样，而是引入了**向量量化 (Vector Quantization, VQ)** 的思想。

#### 架构与机制

1.  **Encoder$E_\phi(x)$:** 输入$x$，输出一个**连续**的特征向量（或特征图）$z_e(x)$。注意，这里不是输出概率或分布参数。
2.  **码本 (Codebook / Embedding Space)$e$:** 维护一个可学习的码本，包含$K$个$D$维的嵌入向量$e_k \in \mathbb{R}^D, k=1...K$。
3.  **量化器 (Quantizer):** 对于编码器输出的$z_e(x)$（如果是特征图，则对每个位置的向量），在码本$e$中找到**距离最近**的码向量$e_k$：
   $k = \underset{j}{\operatorname{argmin}} ||z_e(x) - e_j||_2^2$
    量化后的输出是$z_q(x) = e_k$。这个$e_k$（或其索引$k$）就是 VQ-VAE 的离散表示。
4.  **Decoder$D_\theta(z_q(x))$:** 输入量化后的向量$z_q(x)$，重建原始数据$x$。

#### 解决梯度问题：直通估计器 (STE)

量化步骤（最近邻查找）是不可微的。VQ-VAE 使用了 **直通估计器 (Straight-Through Estimator, STE)** 来近似梯度：

*   **前向传播:** 正常计算$z_q(x) = e_k$。
*   **反向传播:** 当计算损失相对于解码器输入$z_q(x)$的梯度$\nabla_{z_q} L$后，STE 直接将这个梯度**复制**给编码器的输出$z_e(x)$。即，假设$\nabla_{z_e} L \approx \nabla_{z_q} L$。

这相当于在反向传播时“忽略”了量化操作的不可微性，直接让梯度通过。虽然这在数学上是一个近似，但在实践中非常有效，为编码器提供了有用的学习信号，并且避免了 REINFORCE 的高方差问题。

#### 独特的损失函数

VQ-VAE 的总损失函数包含三项：

$L = \underbrace{\log p_\theta(x|z_q(x))}_{\text{Reconstruction Loss}} + \underbrace{|| \text{sg}[z_e(x)] - e ||_2^2}_{\text{VQ Loss}} + \underbrace{\beta || z_e(x) - \text{sg}[e] ||_2^2}_{\text{Commitment Loss}}$

其中 `sg[.]` 代表 `stop-gradient` 操作，意味着其内部的变量不参与梯度计算。

1.  **重建损失 (Reconstruction Loss):** 优化解码器$\theta$和编码器$\phi$（通过 STE 传递梯度）。目标是让解码器能够根据量化后的$z_q(x)$很好地重建$x$。
2.  **VQ (向量量化) 损失 / Codebook Loss:** 只优化码本$e$。目标是让码本向量$e$向编码器实际输出的$z_e(x)$靠拢（使用 stop-gradient 阻止梯度流向编码器）。这实际上是一种 K-Means 聚类的在线版本。
3.  **承诺损失 (Commitment Loss):** 只优化编码器$\phi$。目标是让编码器的输出$z_e(x)$靠近它最终被量化到的那个码本向量$e_k$（使用 stop-gradient 阻止梯度流向码本）。这可以稳定码本的学习，防止编码器输出在码本向量之间跳动过大，并“承诺”使用码本。$\beta$是一个超参数（论文用 0.25）。

**关键点：** 注意这个损失函数中**没有显式的 KL 散度项**$D_{KL}(q(z|x)||p(z))$！

#### 避免后验坍塌

VQ-VAE 通过以下机制有效避免了后验坍塌：

*   **无 KL 压力:** 由于训练 VQ-VAE 时没有 KL 项强制后验接近先验，解码器**必须**利用$z_q(x)$中包含的信息来重建$x$。它没有将 KL 项降为 0 的“捷径”。
*   **Commitment Loss:** 强制编码器输出与选择的码本向量保持一致，进一步确保离散编码被有效利用。
*   **分离先验学习 (Two-stage Training):** VQ-VAE 的常用范式是：
    1.  **第一阶段:** 训练 VQ-VAE (Encoder, Decoder, Codebook)，学习到如何将数据$x$映射到离散的码本索引序列$z$。此时，先验$p(z)$被假定为均匀分布并忽略。
    2.  **第二阶段:** 固定训练好的 VQ-VAE。提取训练集所有数据的离散码序列$z$。然后，训练一个**强大的自回归模型**（如 PixelCNN for images, WaveNet for audio）来直接**学习这些离散码序列的分布$p(z)$**。
    *   **生成:** 采样时，先从训练好的先验$p(z)$中采样一个离散码序列$z$，然后将其输入 VQ-VAE 的解码器生成数据$x$。

这种两阶段方法分离了**学习表示 (VQ-VAE)** 和**学习表示的先验分布 (Autoregressive Model)** 这两个任务，使得每个模型可以专注于自己的任务，非常有效地避免了在联合训练中可能发生的后验坍塌。

---

### Part 4: 对比总结

| 特征                 | 传统 VAE (Continuous)              | 离散 VAE (General, e.g., REINFORCE/Gumbel) | VQ-VAE                                      |
| :------------------- | :--------------------------------- | :--------------------------------------------- | :------------------------------------------ |
| **隐变量类型**       | 连续                             | 离散                                           | 离散 (Codebook Index/Vector)                 |
| **核心机制**         | 重参数化技巧                       | REINFORCE / Gumbel-Softmax 等                | 向量量化, STE, Commitment Loss, Codebook   |
| **梯度估计**         | 精确 (低方差, 无偏)                | 高方差 (REINFORCE) / 有偏 (Gumbel)             | 近似 (STE, 低方差)                          |
| **训练稳定性**       | 相对稳定                           | **不稳定 / 需技巧**                          | **相对稳定**                                |
| **后验坍塌风险**     | 中等                               | **高**                                         | **低** (VQ-VAE 训练阶段), 通过分离先验解决 |
| **主要优点**         | 理论优雅, 训练直接                 | 可以处理离散变量                             | 训练稳定, 离散表示效果好, 不易坍塌        |
| **主要缺点**         | 连续变量限制, 可能坍塌             | 梯度问题, 训练困难, 易坍塌                   | STE 是近似, 需要学习 Codebook              |

---

### 5. 结论

VAE 是一个强大的生成模型框架。然而，当需要学习离散潜在表示时，传统的梯度估计方法面临严峻挑战，导致训练不稳定和后验坍塌。VQ-VAE 通过引入向量量化机制、使用直通估计器近似梯度、设计独特的损失函数以及采用两阶段训练范式，成功地克服了这些困难。它不仅能够稳定地学习到高质量的离散表示，还在图像、音频、视频等多种模态的生成任务上取得了 SOTA 的结果，极大地推动了离散表示学习和生成模型的发展。理解 VQ-VAE 的核心思想对于掌握现代生成模型技术至关重要。

---