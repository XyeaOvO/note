---
create: 2025-04-13 16:40
---
# 论文分享4.15-DepthART：自回归细化单目深度估计

**论文信息:**
*   **Conference/Journal:** arXiv preprint (CV)
*   **Link:** [[2409.15010] DepthART: Monocular Depth Estimation as Autoregressive Refinement Task](https://arxiv.org/abs/2409.15010)
*   **Project Page:** https://bulatko.github.io/depthart-pp/
*   **Code:** [bulatko/DepthART: DepthART official implementation](https://github.com/bulatko/DepthART)

## 1. What is DepthART? 

*   **Task Definition:**
    *   单目深度估计 MDE 任务。

*   **Input & Output:**
    *   **输入:** 单张 RGB **图像**。
    *   **输出:** 对应的**深度图**。**latent space**以多尺度离散 token 图的形式逐步生成和优化深度表示。

*   **Motivation**
	*   传统的监督学习方法（判别式模型）严重依赖标注数据，泛化能力受限，且获取大规模高质量深度标注成本高昂。
	*   最近，基于 **扩散模型 (Diffusion Models)** 的生成方法（如 Marigold, GeoWizard）通过微调预训练模型在 MDE 上取得了 SOTA 效果，展示了生成模型的潜力。
	*   同时，**视觉自回归模型 (Visual Autoregressive, VAR)** 在图像生成上表现出强大的能力。


## 2. What's New in DepthART?

*   **核心创新点 (Core Novelty):**
    *   基于现有的 VAR 架构，开发了**第一个**用于深度估计的自回归 Transformer 模型。
    *   **提出了 DepthART (Depth Autoregressive Refinement Task) 训练范式:** 这是最主要的贡献。它**改变了**传统 VAR 的训练方式。

*   **DepthART:**
* ![](Pic/Pasted%20image%2020250413213021.png)
	1.   **取消 teacher forcing:**
    *   原始 VAR 训练时，输入和目标都依赖于 VQ-VAE 提供的**固定**分解 token map (teacher forcing)。
    *   DepthART 在训练中，使用模型**自身在上一阶段的预测作为下一阶段的输入**，相当于取消了 teacher forcing。
	2.  **动态目标、多模态引导、任务重构:**
		1.  **VAR 的限制：单模态路径**
		    *   如前所述，原始 VAR 的训练目标 `xk` 是由 VQ-VAE 给出的**固定**序列。这代表了 VQ-VAE 找到的**一条特定**的、将连续深度特征 `fD` 分解为离散 token 的路径。
		    *   VAR 模型在训练中，**只被引导去学习和模仿这一条路径**。即使理论上存在其他同样能很好重构 `fD` 的 token 序列（其他有效的“分解模式”），VAR 也不会去学习它们，因为它的损失函数只奖励对 VQ-VAE 给定路径的精确复制。
		    *   因此，VAR 学习到的是一个**单模态 (unimodal)** 的解决方案，被 VQ-VAE 的特定分解方式所限制。
		2.  **DepthART 的机制：允许探索多条路径**
		    *   DepthART 的核心是**动态目标 `tk`**，它基于**当前预测 `z1...z(k-1)`** 和**最终目标 `fD`** 之间的**残差**来计算。
		    *   关键在于：这个目标 `tk` **并不强制要求模型必须遵循 VQ-VAE 给出的那条特定路径**。
		    *   假设模型在第 `i` 步的预测 `zi` 稍微偏离了 VQ-VAE 的标准 `xi`。
		        *   在 VAR 中，下一步的目标仍然是固定的 `x(i+1)`，这可能会将模型强行拉回，或者如果偏离过大，模型可能不知道如何处理。
		        *   在 DepthART 中，下一步的目标 `t(i+1)` 是基于**当前实际残差**计算的。它会告诉模型，**“鉴于你当前的位置（由 `z1...zi` 决定），为了最有效地接近最终目标 `fD`，你应该预测这个 token `t(i+1)`”**。
		    *   这意味着，即使模型走上了一条不同于 VQ-VAE 标准分解的“岔路”，只要这条岔路仍然是有效的（即仍然有可能通过后续步骤较好地逼近 `fD`），DepthART 的动态目标**仍然能提供有意义的、正确的引导**。
        * ![](Pic/Pasted%20image%2020250413213300.png)

![Pasted image 20250316180630](Pic/Pasted%20image%2020250316180630.png)

> 拿这个图来说，之前我的模型是设直接算出了最左边的那一列，再量化得到中间目标，我要预测的就是中间的那一列离散的码本值。
> 现在的话，比如说我处于 loop2，我已经预测出了LOOP1和LOOP2的中间离散值，我应该把他们卷积上采样之后，加起来和原始的最左上角的那个 gt 做差，得到 loop2 等式的右边。
> 也就是说 loop2 等式右边，（loop3 最左边）我模型要预测的目标变成动态的了。

DepthART 的动态目标允许模型在训练中探索不同的、可能更优的**优化路径**，促进了多模态解的发现，提升了最终效果。

这使得模型在训练阶段就能**感知自己的预测**并进行**自我修正和优化**，从而**缩小了训练和推理之间的差距**，减少了推理时错误累积的问题。

## 3. Methodology (方法详解)

![](Pic/Pasted%20image%2020250413213021.png)

*   **整体框架/流程 (Pipeline):**
    1.  **输入:** 训练时输入 RGB 图像 `I` 和对应的真值深度图 `D`。
    2.  **图像编码:** 使用预训练且**冻结**的 VQ-VAE Encoder 将图像 `I` 编码为多尺度**图像 token map** `x = {x1, ..., xK}`，作为 Transformer 的**条件输入**。
    3.  **深度特征提取:** 使用**相同**的 VQ-VAE Encoder 将真值深度图 `D` 编码为连续特征 `fD`（**注意：这里不经过量化**），用于后续计算动态目标。
    4.  **自回归深度生成:**
        *   模型是一个基于 GPT-2 架构的 Transformer (与 VAR 相同)。
        *   在第 `k` 步，Transformer 接收图像条件 `x` 和**先前预测的深度 token** `z1, ..., z(k-1)` 作为输入，预测当前尺度的深度 token `zk`。即 `zk = VAR(z1..zk-1, x1..xK)`。
    5.  **动态目标计算:**
        *   计算真实深度特征 `fD` 与**由预测的 token `z1..z(k-1)` 重建的累积深度**之间的**残差 `δk`** (Eq 6)。
        *   将残差 `δk` 经过缩放 `S` 和量化 `Q` 得到**当前步的目标 token `tk`** (Eq 7)。
    6.  **损失函数:** 最小化**预测的 token `zk`** 和**动态计算的目标 token `tk`** 之间的交叉熵损失 (Eq 8)。

*   **与 VAR 的关键区别:**
    *   VAR 的输入和目标 `tk` 都是由 VQ-VAE 对 GT **直接分解**得到的固定 `xk`。
    *   DepthART 的输入 `z` 是**模型自己生成的**，目标 `tk` 是基于 `fD` 和**模型生成的 `z`** 动态计算的残差。

## 4. Experimental Results (实验与结论)

*   **实验设置:**
    *   **训练数据:** HyperSim (合成室内数据集，因为需要稠密 GT 深度)。
    *   **测试数据:** NYUv 2, IBIMS (室内), TUM (动态室内), ETH 3D (室外)。
    *   **评估指标:** 主要关注**尺度不变** 指标 (AbsRel↓, δ1↑)。也使用了 IBIMS 上的平面几何指标 (pe-fla↓, pe-ori↓)。
    *   **基线模型:**
        *   判别式: MiDaS, GP 2 (EffNet-B 5), DPT (ViT-L), AdaBins。
        *   生成式 (Diffusion): DiT-depth (基于 DiT)。
        *   生成式 (Autoregressive): VAR (原始训练方式)。
    *   **实现细节:** 使用预训练的 VAR (GPT-2 based, 300 M 参数) 和 VQ-VAE。所有模型在 256 x 256 分辨率下训练。

*   **主要结果:**
    *   **VAR vs DepthART:**
        *   在 ETH 3 D 上的重建误差 (AbsRel) 显示，DepthART 在**所有尺度**上都显著优于原始 VAR 训练方式，相对误差降低约 70%。
        *   DepthART 预测的 token 熵更高，验证了其促进多模态路径探索的能力。
        * ![](Pic/Pasted%20image%2020250413223130.png)
    *   **与基线对比 (Table 1):**
        *   DepthART 在所有测试集上的**平均排名最高 (2.1)**，表明其综合性能最优。
        *   在多个指标上达到 SOTA 或具有竞争力。
        *   尤其在 IBIMS 的**平面精度 (pe-fla, pe-ori)** 指标上表现突出，优于其他方法，说明其生成的深度具有更好的几何一致性。
        * ![](Pic/Pasted%20image%2020250413223324.png)
    *   **定性结果 (Qualitative Results - Fig 5, 6, 7, 8, 9, 10):**
        *   生成的深度图细节更丰富，尤其在平面区域更准确平滑 (Fig 6)。
        * ![](Pic/Pasted%20image%2020250413223411.png)
        *   重建的点云质量更高，几何结构更清晰 (Fig 5, 8, 9)。
        * ![](Pic/Pasted%20image%2020250413223341.png)
        *   展示了逐尺度优化的过程 (Fig 7)。
        * ![](Pic/Pasted%20image%2020250413223441.png)
        *   在未见过的真实世界图像 (COCO) 上也展现了较好的泛化能力 (Fig 10)。
        * ![](Pic/Pasted%20image%2020250413223507.png)

*   **实验结论:**
    *   DepthART 训练范式显著提升了自回归 Transformer 在单目深度估计任务上的性能。
    *   相比原始 VAR，DepthART 效果更好；相比其他判别式和扩散模型基线，DepthART 具有竞争力或更优，尤其是在几何精度方面。
    *   证明了自回归模型在深度估计领域的潜力。

## 5. Bring Home Message (总结与启发)

*   **核心总结:**
    *   这篇论文展示了如何通过改进**训练范式 (DepthART)**，将**自回归生成模型**成功应用于具有挑战性的**单目深度估计**任务，并通过**自优化**和**多模态学习**实现了与 SOTA 方法相当甚至更好的性能，尤其是在保持几何结构方面。

*   **优点:**
    *   提出了一种新颖有效的自回归模型训练方法 (DepthART)，解决了训练与推理不一致的问题。
    *   生成的深度图质量高，几何一致性好（平面区域表现佳）。
    *   展示了自回归模型在 MDE 任务上的潜力，是 Diffusion 之外的另一条有前景的技术路线。
    *   可以利用现有的预训练自回归模型（如 VAR）。

*   **缺点/局限性:**
    *   **依赖预训练 VQ-VAE:** 性能受 VQ-VAE 质量的限制。文中使用的 VQ-VAE 是在 ImageNet 上预训练且分辨率较低 (256 x 256)，并非针对深度数据优化，可能存在 VQ-VAE 编码/解码的瓶颈和不稳定性 (Appendix 提到)。升级 VQ-VAE 可能会带来提升。
    *   **训练数据:** 仅在合成数据集 (HyperSim) 上训练，在真实数据上的泛化能力可能需要进一步验证或通过微调提升。
    *   **推理速度:** 自回归模型的推理通常是串行的（逐尺度或逐 token），相对于前向一次性推理的判别式模型可能较慢（但可能比 Diffusion 采样快）。

*   **对我们的启发 (Inspiration for Us / What can we learn?):**
    *   **自优化/残差学习思想:** DepthART 的核心思想——在训练中利用模型自身的预测来构建动态目标和进行残差学习，这种**自优化 (self-refinement)** 的思路可以借鉴到我们自己的法线生成任务中。
    *   **训练与推理一致性:** 对于序列生成或自回归类模型，如何设计训练过程来更好地模拟推理过程，减少 gap，是一个值得思考的问题。DepthART 提供了一个范例。
    *   **探索 AR 模型潜力:** 除了 CNN, Transformer (Encoder-Decoder), Diffusion Models，自回归 Decoder-only Transformer 在视觉任务中（尤其是生成和估计类任务）也值得关注和探索。
    *   **评估指标:** **平面精度 (pe-fla, pe-ori)** 指标

## 6. Q&A

*   欢迎提问和讨论！
